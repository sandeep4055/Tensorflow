{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Tape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***tf.GradientTape*** is an API in TensorFlow that provides automatic differentiation for computing gradients. It is primarily used in the context of training neural networks and optimizing model parameters.\n",
    "\n",
    "- By using tf.GradientTape, you can compute gradients of any TensorFlow operation with respect to the variables used in the computation. This enables automatic differentiation and facilitates the training of models through backpropagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a step-by-step example of how to use tf.GradientTape for differentiation:\n",
    "\n",
    "1. Define your function or computation that you want to differentiate. This could be a loss function, a model's forward pass, or any custom computation.\n",
    "\n",
    "2. Start a gradient tape block using tf.GradientTape(). This will begin recording operations for gradient computation.\n",
    "\n",
    "3. Perform your computation within the tape block, ensuring that you use TensorFlow operations and variables. TensorFlow will trace the operations and record the gradients for each variable.\n",
    "\n",
    "4. Calculate the gradients by calling the gradient method on the tape object, specifying the target variable(s) for differentiation. This will return the gradients with respect to the specified variables.\n",
    "\n",
    "5. Use the gradients for further computations, such as updating model parameters through an optimizer or analyzing the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Here's an example to illustrate the usage of tf.GradientTape:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a function for differentiation\n",
    "def my_function(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "# Define the input variable\n",
    "x = tf.Variable(5.0)\n",
    "\n",
    "# Start the gradient tape block\n",
    "with tf.GradientTape() as tape:\n",
    "    # Perform the computation within the tape block\n",
    "    y = my_function(x)\n",
    "\n",
    "# Calculate the gradient of y with respect to x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(dy_dx)  # Output: tf.Tensor(6.0, shape=(), dtype=float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, we define a simple quadratic function my_function(x) = x**2 + 2*x + 1 and compute its gradient with respect to the input variable x. The resulting gradient is 6.0, indicating the rate of change of the function with respect to x at the given value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tape.gradient\n",
    "\n",
    "- The tape.gradient(target, sources) method is used in TensorFlow to compute the gradients of a target tensor with respect to a list of source tensors. It is a key component of automatic differentiation, allowing us to compute the gradients of a computational graph.\n",
    "\n",
    "- The target is the tensor we want to compute the gradients for, and the sources are the tensors with respect to which we want to compute the gradients. The target and sources should be differentiable tensors, meaning that TensorFlow can automatically track their operations and compute their gradients.\n",
    "\n",
    "##### Here's the general syntax of using tape.gradient():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = tf.constant(3.0)\n",
    "\n",
    "# Create a tape\n",
    "with tf.GradientTape() as tape:\n",
    "    # Watch the tensor\n",
    "    tape.watch(x)\n",
    "\n",
    "    # Perform some computations\n",
    "    y = x ** 2\n",
    "\n",
    "# Compute the gradient of y with respect to x\n",
    "gradient = tape.gradient(y, x)\n",
    "\n",
    "print(gradient)  # Output: tf.Tensor(6.0, shape=(), dtype=float32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, we create a tensor x with a value of 3.0. Inside the tf.GradientTape() context, we perform some computations on x to get y = x ** 2. By calling tape.gradient(y, x), we compute the gradient of y with respect to x, which is 6.0 in this case.\n",
    "\n",
    "- Note that the tape.gradient() method can handle more complex computations involving multiple tensors and operations. It computes the gradients using the chain rule of calculus, propagating the gradients backward through the computational graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tape.watch\n",
    "\n",
    "- The tape.watch() function in TensorFlow's tf.GradientTape allows you to explicitly mark a tensor for gradient computation, even if it's not used in the forward pass. This can be useful in scenarios where you want to compute gradients with respect to certain tensors that are not part of the standard computation graph.\n",
    "\n",
    "##### Here's a real-world example to help illustrate the use of tape.watch():\n",
    "\n",
    "- Suppose you have a pre-trained convolutional neural network (CNN) model, and you want to fine-tune it on a new task by updating the weights of only a specific subset of layers. In this case, you need to compute the gradients with respect to those specific layers' weights.\n",
    "\n",
    "- To do this, you can use tape.watch() to explicitly mark the tensors corresponding to the weights of the layers you want to update. By marking these tensors, TensorFlow will compute the gradients with respect to them during the backward pass, even if they are not used in the forward pass.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pre-trained CNN model\n",
    "model = tf.keras.applications.ResNet50(weights='imagenet')\n",
    "\n",
    "# Get the subset of layers you want to fine-tune\n",
    "layers_to_train = model.layers[-3:]  # Example: Fine-tuning the last 3 layers\n",
    "\n",
    "# Create a gradient tape\n",
    "with tf.GradientTape() as tape:\n",
    "    # Enable tape to watch the weights of the layers to train\n",
    "    for layer in layers_to_train:\n",
    "        tape.watch(layer.weights)\n",
    "\n",
    "    # Create some sample input data\n",
    "    input_data = tf.random.normal(shape=(1, 224, 224, 3))\n",
    "\n",
    "    # Perform forward pass with your input data\n",
    "    output = model(input_data)\n",
    "\n",
    "# Compute gradients with respect to the watched weights\n",
    "gradients = tape.gradient(output, [layer.weights for layer in layers_to_train])\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Update the weights using gradients and the optimizer\n",
    "for layer, grads in zip(layers_to_train, gradients):\n",
    "    optimizer.apply_gradients(zip(grads, layer.trainable_weights))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persistent=True\n",
    "\n",
    "- In TensorFlow, the persistent=True option is used in combination with the tf.GradientTape context to enable persistent mode. By default, TensorFlow's gradient tape records operations only once, and the tape is discarded as soon as the gradient() method is called to compute gradients. However, when persistent=True is set, the tape is kept even after gradient computation, allowing multiple gradient computations to be performed on the same tape.\n",
    "\n",
    "##### Here's how persistent=True can be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x: 6.0\n",
      "Gradient of z with respect to x: 108.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "    z = y**2\n",
    "\n",
    "grad1 = tape.gradient(y, x)\n",
    "grad2 = tape.gradient(z, x)\n",
    "\n",
    "print(\"Gradient of y with respect to x:\", grad1.numpy())  # Output: 6.0\n",
    "print(\"Gradient of z with respect to x:\", grad2.numpy())  # Output: 108.0\n",
    "\n",
    "del tape  # Remember to delete the tape when no longer needed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above example, by setting persistent=True in the tf.GradientTape context, we can compute gradients of both y and z with respect to x using the same tape. Without persistent=True, the tape would be discarded after the first gradient computation, and we would not be able to compute the second gradient.\n",
    "\n",
    "- Persistent mode can be useful in scenarios where you need to compute gradients multiple times or when you want to access intermediate results computed during the forward pass. However, it's important to note that using persistent=True can consume additional memory, as the tape is kept in memory until it is deleted explicitly. Therefore, it's recommended to delete the tape (del tape) when you no longer need it to free up memory resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
