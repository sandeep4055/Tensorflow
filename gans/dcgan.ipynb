{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN - Deep Convolutional GANs\n",
    "\n",
    "- [DCGAN paper link](https://arxiv.org/abs/1511.06434)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DCGAN (Deep Convolutional Generative Adversarial Network) has several unique features that distinguish it from traditional GAN architectures. Here are some of the key characteristics of DCGAN:\n",
    "\n",
    "- Convolutional Architecture: DCGAN utilizes convolutional layers in both the generator and discriminator networks instead of fully connected layers. This allows the model to effectively capture spatial information and generate high-resolution images.\n",
    "\n",
    "- Strided Convolutions and Transposed Convolutions: DCGAN uses strided convolutions in the discriminator to downsample the input and transposed convolutions in the generator to upsample the noise input. This helps in learning hierarchical representations and generating higher-resolution images.\n",
    "\n",
    "- Batch Normalization: DCGAN applies batch normalization to the activations in both the generator and discriminator networks. It helps in stabilizing the training process and accelerates convergence by normalizing the inputs to each layer.\n",
    "\n",
    "- LeakyReLU Activation: DCGAN employs LeakyReLU activation in the discriminator network instead of traditional ReLU. LeakyReLU allows for non-zero gradients for negative inputs, preventing the \"dying ReLU\" problem and improving the flow of gradients during training.\n",
    "\n",
    "- No Fully Connected Layers: DCGAN does not use fully connected layers in the discriminator or generator. Instead, it relies on convolutional layers and global pooling to reduce the spatial dimensions of the activations.\n",
    "\n",
    "- Random Noise Input: DCGAN takes random noise as input to the generator network. This noise vector is typically sampled from a uniform or normal distribution and serves as the source of randomness for generating diverse images.\n",
    "\n",
    "These unique characteristics make DCGAN well-suited for generating high-quality and realistic images. It has been widely used in various applications, including image synthesis, image-to-image translation, and style transfer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def make_generator_model():\n",
    "    inputs = tf.keras.Input(shape=(100,))\n",
    "    x = layers.Dense(7*7*256, use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Reshape((7, 7, 256))(x)\n",
    "    assert x.shape == (None, 7, 7, 256)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    assert x.shape == (None, 7, 7, 128)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    assert x.shape == (None, 14, 14, 64)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "    assert x.shape == (None, 28, 28, 1)\n",
    "\n",
    "    generator = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def make_discriminator_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(inputs)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    discriminator = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator and discriminator models\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator loss\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([tf.shape(images)[0], 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Loading & preprocessing\n",
    "\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        if epoch:\n",
    "\n",
    "            print(f\"Current epoch is {epoch}\")\n",
    "\n",
    "            # Generate samples after each epoch\n",
    "            random_latent_vectors = tf.random.normal([10, 100])\n",
    "            generated_images = generator(random_latent_vectors, training=False)\n",
    "\n",
    "            # Reshape the generated images\n",
    "            generated_images = generated_images.numpy().reshape(-1, 28, 28)\n",
    "\n",
    "            # Display the generated images\n",
    "            plt.figure(figsize=(10, 1))\n",
    "            for i in range(10):\n",
    "                plt.subplot(1, 10, i + 1)\n",
    "                plt.imshow(generated_images[i], cmap='gray')\n",
    "                plt.axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
