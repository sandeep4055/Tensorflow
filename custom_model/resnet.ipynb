{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ResNet, short for \"Residual Network,\" is a type of neural network architecture designed to address the vanishing gradient problem that occurs when training very deep neural networks. The vanishing gradient problem occurs when the gradient signal becomes too weak during backpropagation, making it difficult for the network to learn and update the weights of earlier layers.\n",
    "\n",
    "- ResNet introduces the concept of residual learning, where shortcut connections are added to the network that bypass one or more layers. This allows the gradient signal to be directly propagated to earlier layers, which helps alleviate the vanishing gradient problem and allows for training deeper networks.\n",
    "\n",
    "- The ResNet architecture consists of a series of blocks, each containing multiple convolutional layers and shortcut connections. The shortcut connections can take several forms, including identity mappings or projection mappings that match the dimensions of the input and output feature maps.\n",
    "\n",
    "- The original ResNet paper, published in 2015, introduced several variants of the ResNet architecture, including ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. These variants differ in their depth and number of parameters, with ResNet-50 being one of the most commonly used architectures in computer vision tasks.\n",
    "\n",
    "- ResNet, is a deep neural network architecture that was introduced in 2015 by Kaiming He et al. in their paper \"Deep Residual Learning for Image Recognition\". It is an extension of the previous state-of-the-art architecture, the GoogLeNet (Inception v1), and it brought a significant improvement in the ability to train very deep neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet 18 :\n",
    "- The residual network has multiple variations, namely ResNet16, ResNet18, ResNet34, ResNet50, ResNet101, ResNet110, ResNet152, ResNet164, ResNet1202, and so forth.\n",
    "\n",
    "- ResNet18 is a 72-layer architecture with 18 deep layers. The architecture of this network aimed at enabling large amounts of convolutional layers to function efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "#### Here's an explanation of the ResNet-18 architecture in a stepwise manner:\n",
    "\n",
    "1. The input to the ResNet-18 architecture is a 224 x 224 x 3 RGB image.\n",
    "\n",
    "2. The first layer is a convolutional layer with 64 filters of size 3 x 3, with a stride of 1 and 'same' padding.\n",
    "\n",
    "3. This is followed by a batch normalization layer and a ReLU activation function.\n",
    "\n",
    "4. The next layer is a max pooling layer with a pool size of 3 x 3 and a stride of 2.\n",
    "\n",
    "5. This is followed by a sequence of 4 residual blocks, each of which contains two convolutional layers, each with 64 filters of size 3 x 3, and a batch normalization layer and a ReLU activation function after each convolution. The first convolutional layer in each block has a stride of 1, while the second has a stride of 1 or 2, depending on whether the input and output feature map sizes are the same or different.\n",
    "\n",
    "6. The first residual block also has a shortcut connection that bypasses the two convolutional layers and adds the input directly to the output of the second convolutional layer.\n",
    "\n",
    "7. The output of the last residual block is passed through a global average pooling layer, which computes the average of each feature map across its spatial dimensions, resulting in a 1 x 1 x 512 feature map.\n",
    "\n",
    "8. This is followed by a fully connected layer with 1000 units, which corresponds to the number of classes in the ImageNet dataset.\n",
    "\n",
    "9. Finally, a softmax activation function is applied to the output of the fully connected layer to obtain the class probabilities.\n",
    "\n",
    "Overall, the ResNet-18 architecture is a deep convolutional neural network that uses residual connections to allow for the training of very deep networks. It was designed for the ImageNet classification task, but can be adapted for other tasks as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Resnet 18](resnet-18.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Blocks\n",
    "\n",
    "- Residual blocks are the building blocks of ResNet (Residual Network) architectures. They were introduced to overcome the problem of vanishing gradients in deep neural networks. In a residual block, the input to the block is added to its output. This input and output addition is known as a skip connection or shortcut connection. The skip connection allows the network to learn the residual mapping (the difference between input and output) instead of the direct mapping from input to output. This makes it easier for the network to learn the identity function, which is useful when the input and output have the same dimensions.\n",
    "\n",
    "![residual bloack](residual_bloacks.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
